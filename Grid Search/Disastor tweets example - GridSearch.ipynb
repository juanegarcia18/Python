{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Communal violence in Bhainsa, Telangana. \"Ston...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Telangana: Section 144 has been imposed in Bha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Morgantown, WV</td>\n",
       "      <td>Arsonist sets cars ablaze at dealership https:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Lord Jesus, your love brings freedom and pard...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3yrs after IPOB formed Biafra security Service...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>www</td>\n",
       "      <td>France agrees to send more troops to West Afri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>USA</td>\n",
       "      <td>While the press feasts off a tiny \"he-said, sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>North Pole</td>\n",
       "      <td>● NEWS ● #meduza #russia ☞ Man who made Russia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>terrorism</td>\n",
       "      <td>UK</td>\n",
       "      <td>Donald Trump posted a doctored islamophobia ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    keyword        location  \\\n",
       "0        0     ablaze             NaN   \n",
       "1        1     ablaze             NaN   \n",
       "2        2     ablaze   New York City   \n",
       "3        3     ablaze  Morgantown, WV   \n",
       "4        4     ablaze             NaN   \n",
       "...    ...        ...             ...   \n",
       "9995  9995  terrorism             NaN   \n",
       "9996  9996  terrorism             www   \n",
       "9997  9997  terrorism             USA   \n",
       "9998  9998  terrorism      North Pole   \n",
       "9999  9999  terrorism              UK   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Communal violence in Bhainsa, Telangana. \"Ston...       1  \n",
       "1     Telangana: Section 144 has been imposed in Bha...       1  \n",
       "2     Arsonist sets cars ablaze at dealership https:...       1  \n",
       "3     Arsonist sets cars ablaze at dealership https:...       1  \n",
       "4     \"Lord Jesus, your love brings freedom and pard...       0  \n",
       "...                                                 ...     ...  \n",
       "9995  3yrs after IPOB formed Biafra security Service...       0  \n",
       "9996  France agrees to send more troops to West Afri...       1  \n",
       "9997  While the press feasts off a tiny \"he-said, sh...       0  \n",
       "9998  ● NEWS ● #meduza #russia ☞ Man who made Russia...       0  \n",
       "9999  Donald Trump posted a doctored islamophobia ph...       0  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"train_disastor.csv\", # the location to the data file\n",
    "                       sep=\",\", nrows = 10000 # for tab delimited documents, use \"\\t\" as the seperator\n",
    "                       #names=[\"user ID\", \"book ID\", \"rating\"] # define the names for the three columns\n",
    "                       )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>007npen6lg</th>\n",
       "      <th>00cy9vxeff</th>\n",
       "      <th>00end</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>0215</th>\n",
       "      <th>...</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûò800000</th>\n",
       "      <th>ûòthe</th>\n",
       "      <th>ûòåêcnbc</th>\n",
       "      <th>ûó</th>\n",
       "      <th>ûóher</th>\n",
       "      <th>ûókody</th>\n",
       "      <th>ûónegligence</th>\n",
       "      <th>ûótech</th>\n",
       "      <th>ûówe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 21363 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  0000  007npen6lg  00cy9vxeff  00end  00pm  01  02  0215  ...  \\\n",
       "0      0    0     0           0           0      0     0   0   0     0  ...   \n",
       "1      0    0     0           0           0      0     0   0   0     0  ...   \n",
       "2      0    0     0           0           0      0     0   0   0     0  ...   \n",
       "3      0    1     0           0           0      0     0   0   0     0  ...   \n",
       "4      0    0     0           0           0      0     0   0   0     0  ...   \n",
       "...   ..  ...   ...         ...         ...    ...   ...  ..  ..   ...  ...   \n",
       "7608   0    0     0           0           0      0     0   0   0     0  ...   \n",
       "7609   0    0     0           0           0      0     0   0   0     0  ...   \n",
       "7610   0    0     0           0           0      0     0   1   0     0  ...   \n",
       "7611   0    0     0           0           0      0     0   0   0     0  ...   \n",
       "7612   0    0     0           0           0      0     0   0   0     0  ...   \n",
       "\n",
       "      ûò  ûò800000  ûòthe  ûòåêcnbc  ûó  ûóher  ûókody  ûónegligence  ûótech  \\\n",
       "0      0         0      0         0   0      0       0             0       0   \n",
       "1      0         0      0         0   0      0       0             0       0   \n",
       "2      0         0      0         0   0      0       0             0       0   \n",
       "3      0         0      0         0   0      0       0             0       0   \n",
       "4      0         0      0         0   0      0       0             0       0   \n",
       "...   ..       ...    ...       ...  ..    ...     ...           ...     ...   \n",
       "7608   0         0      0         0   0      0       0             0       0   \n",
       "7609   0         0      0         0   0      0       0             0       0   \n",
       "7610   0         0      0         0   0      0       0             0       0   \n",
       "7611   0         0      0         0   0      0       0             0       0   \n",
       "7612   0         0      0         0   0      0       0             0       0   \n",
       "\n",
       "      ûówe  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "...    ...  \n",
       "7608     0  \n",
       "7609     0  \n",
       "7610     0  \n",
       "7611     0  \n",
       "7612     0  \n",
       "\n",
       "[7613 rows x 21363 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "vectorizer = CountVectorizer(binary=True, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(dataset[\"text\"])\n",
    "df_tf = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "df_tf # predictors (independent variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "(5329, 21363)\n",
      "(2284, 21363)\n",
      "(5329, 1)\n",
      "(2284, 1)\n",
      "class counts\n",
      "0    4342\n",
      "1    3271\n",
      "Name: target, dtype: int64\n",
      "0    3029\n",
      "1    2300\n",
      "Name: target, dtype: int64\n",
      "0    1313\n",
      "1     971\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Partition the data set\n",
    "\n",
    "from sklearn.model_selection  import train_test_split\n",
    "\n",
    "# partition: train/test = 70/30\n",
    "train_x, test_x, train_y, test_y = train_test_split(df_tf, dataset[\"target\"], test_size=0.3, random_state=123)\n",
    "\n",
    "# convert numpy arrays to data frames\n",
    "df_train_x = pd.DataFrame(train_x, columns=df_tf.columns)\n",
    "df_test_x = pd.DataFrame(test_x, columns=df_tf.columns)\n",
    "df_train_y = pd.DataFrame(train_y, columns=[\"target\"])\n",
    "df_test_y = pd.DataFrame(test_y, columns=[\"target\"])\n",
    "\n",
    "print (\"shapes\")\n",
    "print (df_train_x.shape)\n",
    "print (df_test_x.shape)\n",
    "print (df_train_y.shape)\n",
    "print (df_test_y.shape)\n",
    "\n",
    "print \n",
    "\n",
    "print (\"class counts\")\n",
    "print (dataset[\"target\"].value_counts())\n",
    "print (df_train_y[\"target\"].value_counts())\n",
    "print (df_test_y[\"target\"].value_counts())\n",
    "# the class counts show that the sampling is roughly stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.6977992485238862\n",
      "accuracy:0.7535026269702276\n",
      "precision:0.6694129763130793\n",
      "recall:0.7286995515695067\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "# train model\n",
    "clf = clf.fit(train_x, train_y)\n",
    "# make prediction\n",
    "pred_y = clf.predict(test_x)#pred_y=[1,2,2,1,..........,2]\n",
    "# evaluate the prediction results\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print (\"f1:\" + str(f1_score(pred_y, test_y)))\n",
    "print (\"accuracy:\" + str(accuracy_score(pred_y, test_y)))\n",
    "print (\"precision:\" + str(precision_score(pred_y, test_y)))\n",
    "print (\"recall:\" + str(recall_score(pred_y, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.7311015118790497\n",
      "accuracy:0.7819614711033275\n",
      "precision:0.6972193614830072\n",
      "recall:0.7684449489216799\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf = clf.fit(df_train_x, train_y)\n",
    "#df_importance = pd.DataFrame(zip(df_train_x.columns, clf.coef_[0]), columns=[\"feature\", \"weight\"])\n",
    "#df_importance.sort_values(\"weight\", ascending=False)\n",
    "pred_y = clf.predict(df_test_x)\n",
    "print (\"f1:\" + str(f1_score(pred_y, test_y)))\n",
    "print (\"accuracy:\" + str(accuracy_score(pred_y, test_y)))\n",
    "print (\"precision:\" + str(precision_score(pred_y, test_y)))\n",
    "print (\"recall:\" + str(recall_score(pred_y, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.7549947423764459\n",
      "accuracy:0.7959719789842382\n",
      "precision:0.7394438722966015\n",
      "recall:0.7712137486573577\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "# train model\n",
    "clf = clf.fit(train_x, train_y)\n",
    "# make prediction\n",
    "pred_y = clf.predict(test_x)\n",
    "# evaluate the prediction results\n",
    "print (\"f1:\" + str(f1_score(pred_y, test_y)))\n",
    "print (\"accuracy:\" + str(accuracy_score(pred_y, test_y)))\n",
    "print (\"precision:\" + str(precision_score(pred_y, test_y)))\n",
    "print (\"recall:\" + str(recall_score(pred_y, test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramah\\Anaconda2\\envs\\ipykernel_py3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:0.7487179487179486\n",
      "accuracy:0.8069176882661997\n",
      "precision:0.6766220391349125\n",
      "recall:0.8380102040816326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "# train model\n",
    "clf = clf.fit(train_x, train_y)\n",
    "# make prediction\n",
    "pred_y = clf.predict(test_x)\n",
    "# evaluate the prediction results\n",
    "print (\"f1:\" + str(f1_score(pred_y, test_y)))\n",
    "print (\"accuracy:\" + str(accuracy_score(pred_y, test_y)))\n",
    "print (\"precision:\" + str(precision_score(pred_y, test_y)))\n",
    "print (\"recall:\" + str(recall_score(pred_y, test_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and Fine-Tune Your Model with feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.92      0.78       868\n",
      "           1       0.81      0.42      0.56       655\n",
      "\n",
      "    accuracy                           0.71      1523\n",
      "   macro avg       0.74      0.67      0.67      1523\n",
      "weighted avg       0.74      0.71      0.69      1523\n",
      "\n",
      "Fold 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.84      0.75       869\n",
      "           1       0.69      0.45      0.55       654\n",
      "\n",
      "    accuracy                           0.68      1523\n",
      "   macro avg       0.68      0.65      0.65      1523\n",
      "weighted avg       0.68      0.68      0.66      1523\n",
      "\n",
      "Fold 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.87      0.75       869\n",
      "           1       0.70      0.41      0.52       654\n",
      "\n",
      "    accuracy                           0.67      1523\n",
      "   macro avg       0.68      0.64      0.63      1523\n",
      "weighted avg       0.68      0.67      0.65      1523\n",
      "\n",
      "Fold 4\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.77       868\n",
      "           1       0.76      0.43      0.55       654\n",
      "\n",
      "    accuracy                           0.70      1522\n",
      "   macro avg       0.72      0.67      0.66      1522\n",
      "weighted avg       0.71      0.70      0.68      1522\n",
      "\n",
      "Fold 5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.90      0.83       868\n",
      "           1       0.82      0.65      0.73       654\n",
      "\n",
      "    accuracy                           0.79      1522\n",
      "   macro avg       0.80      0.77      0.78      1522\n",
      "weighted avg       0.80      0.79      0.79      1522\n",
      "\n",
      "Average F1: 0.58\n",
      "Average prcesion: 0.76\n",
      "Average recall: 0.47\n",
      "Average accuracy: 0.71\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#cross validation logistic regression\n",
    "#Grid Search-improve accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import *\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fold = 0\n",
    "f1 = []\n",
    "precision =[]\n",
    "recall=[]\n",
    "accuracy=[]\n",
    "features = []\n",
    "for train_index, test_index in skf.split(df_tf,dataset[\"target\"]):\n",
    "    fold += 1\n",
    "    print (\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = df_tf.iloc[train_index], df_tf.iloc[test_index]\n",
    "    train_y, test_y = dataset[\"target\"].iloc[train_index], dataset[\"target\"].iloc[test_index]\n",
    "    \n",
    "    # vectorize\n",
    "    #vectorizer = CountVectorizer(tokenizer=tokenize, binary=True, stop_words='english')\n",
    "    #X = vectorizer.fit_transform(train_x)\n",
    "    X = train_x\n",
    "    #X_test = vectorizer.transform(test_x)\n",
    "    X_test = test_x\n",
    "    \n",
    "    #X = vectorizer.fit_transform(dataset[\"ReviewText\"])\n",
    "    #df_tf = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # convert numpy arrays to data frames\n",
    "    df_train_x = pd.DataFrame(train_x, columns=df_tf.columns)\n",
    "    df_test_x = pd.DataFrame(test_x, columns=df_tf.columns)\n",
    "    df_train_y = pd.DataFrame(train_y, columns=[\"target\"])\n",
    "    df_test_y = pd.DataFrame(test_y, columns=[\"target\"])\n",
    "    \n",
    "    #feature selection\n",
    "    f_val, p_val = chi2(df_train_x, df_train_y[\"target\"]) \n",
    "    #f_val, p_val = chi2(train_x, train_y[\"ReviewRate \"]) \n",
    "\n",
    "    # print the Chi-squared valus and p values\n",
    "    df_scores = pd.DataFrame(zip(df_tf.columns, f_val, p_val), columns=[\"feature\", \"chi2\", \"p\"])\n",
    "    df_scores[\"chi2\"] = df_scores[\"chi2\"].round(2)\n",
    "    df_scores[\"p\"] = df_scores[\"p\"].round(3)\n",
    "    #print df_scores.sort_values(\"chi2\", ascending=False)\n",
    "\n",
    "    # use features with p < 0.05\n",
    "    sel_ohe_cols = df_scores[df_scores[\"p\"]<0.05][\"feature\"].values\n",
    "    \n",
    "    # train model\n",
    "    \n",
    "    clf = LogisticRegression(random_state=fold)\n",
    "    grid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\n",
    "    grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'accuracy')\n",
    "    grid_clf_acc.fit(X[sel_ohe_cols], train_y)\n",
    "    #grid_clf_acc.fit(X, train_y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #clf.fit(X[sel_ohe_cols], train_y)\n",
    "    # predict\n",
    "    pred=grid_clf_acc.predict(X_test[sel_ohe_cols])\n",
    "    #pred = clf.predict(X_test[sel_ohe_cols])\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print (line)\n",
    "    f1.append(metrics.f1_score(test_y, pred))\n",
    "    precision.append(metrics.precision_score(test_y, pred))\n",
    "    recall.append(metrics.recall_score(test_y, pred))\n",
    "    accuracy.append(metrics.accuracy_score(test_y, pred))\n",
    "    #features.append(len(vectorizer.vocabulary_))\n",
    "    \n",
    "print (\"Average F1: %.2f\" % np.mean(f1))\n",
    "print (\"Average prcesion: %.2f\" % np.mean(precision))\n",
    "print (\"Average recall: %.2f\" % np.mean(recall))\n",
    "print (\"Average accuracy: %.2f\" % np.mean(accuracy))\n",
    "#print (\"Average F1: %.2f\" % np.mean(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "#cross validation logistic regression\n",
    "#Grid Search-improve accuracy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.feature_selection import *\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "fold = 0\n",
    "f1 = []\n",
    "precision =[]\n",
    "recall=[]\n",
    "accuracy=[]\n",
    "features = []\n",
    "for train_index, test_index in skf.split(df_tf,dataset[\"target\"]):\n",
    "    fold += 1\n",
    "    print (\"Fold %d\" % fold)\n",
    "    # partition\n",
    "    train_x, test_x = df_tf.iloc[train_index], df_tf.iloc[test_index]\n",
    "    train_y, test_y = dataset[\"target\"].iloc[train_index], dataset[\"target\"].iloc[test_index]\n",
    "    \n",
    "    # vectorize\n",
    "    #vectorizer = CountVectorizer(tokenizer=tokenize, binary=True, stop_words='english')\n",
    "    #X = vectorizer.fit_transform(train_x)\n",
    "    X = train_x\n",
    "    #X_test = vectorizer.transform(test_x)\n",
    "    X_test = test_x\n",
    "    \n",
    "    #X = vectorizer.fit_transform(dataset[\"ReviewText\"])\n",
    "    #df_tf = pd.DataFrame(X.todense(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # convert numpy arrays to data frames\n",
    "    df_train_x = pd.DataFrame(train_x, columns=df_tf.columns)\n",
    "    df_test_x = pd.DataFrame(test_x, columns=df_tf.columns)\n",
    "    df_train_y = pd.DataFrame(train_y, columns=[\"target\"])\n",
    "    df_test_y = pd.DataFrame(test_y, columns=[\"target\"])\n",
    "    \n",
    "    #feature selection\n",
    "    f_val, p_val = chi2(df_train_x, df_train_y[\"target\"]) \n",
    "    #f_val, p_val = chi2(train_x, train_y[\"ReviewRate \"]) \n",
    "\n",
    "    # print the Chi-squared valus and p values\n",
    "    df_scores = pd.DataFrame(zip(df_tf.columns, f_val, p_val), columns=[\"feature\", \"chi2\", \"p\"])\n",
    "    df_scores[\"chi2\"] = df_scores[\"chi2\"].round(2)\n",
    "    df_scores[\"p\"] = df_scores[\"p\"].round(3)\n",
    "    #print df_scores.sort_values(\"chi2\", ascending=False)\n",
    "\n",
    "    # use features with p < 0.05\n",
    "    sel_ohe_cols = df_scores[df_scores[\"p\"]<0.05][\"feature\"].values\n",
    "    \n",
    "    # train model\n",
    "    clf = svm.SVC(random_state=fold)\n",
    "    #clf = LogisticRegression(random_state=fold)\n",
    "    grid_values = {'kernel': ['linear', 'poly', 'rbf'],'C':[0.1,0.01,1,5,10,25]}\n",
    "    grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'accuracy')\n",
    "    grid_clf_acc.fit(X[sel_ohe_cols], train_y)\n",
    "    #grid_clf_acc.fit(X, train_y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #clf.fit(X[sel_ohe_cols], train_y)\n",
    "    # predict\n",
    "    pred=grid_clf_acc.predict(X_test[sel_ohe_cols])\n",
    "    #pred = clf.predict(X_test[sel_ohe_cols])\n",
    "    # classification results\n",
    "    for line in metrics.classification_report(test_y, pred).split(\"\\n\"):\n",
    "        print (line)\n",
    "    f1.append(metrics.f1_score(test_y, pred))\n",
    "    precision.append(metrics.precision_score(test_y, pred))\n",
    "    recall.append(metrics.recall_score(test_y, pred))\n",
    "    accuracy.append(metrics.accuracy_score(test_y, pred))\n",
    "    #features.append(len(vectorizer.vocabulary_))\n",
    "    \n",
    "print (\"Average F1: %.2f\" % np.mean(f1))\n",
    "print (\"Average prcesion: %.2f\" % np.mean(precision))\n",
    "print (\"Average recall: %.2f\" % np.mean(recall))\n",
    "print (\"Average accuracy: %.2f\" % np.mean(accuracy))\n",
    "#print (\"Average F1: %.2f\" % np.mean(features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "8636878d62b8f8bda25dd92670d5a9df30107a7d09efd2b95a0c0e53f2429c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
